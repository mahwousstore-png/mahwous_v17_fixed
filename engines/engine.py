"""
engines/engine.py  Â·  Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù‡Ø¬ÙŠÙ†  v20.0
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ù…Ø±Ø­Ù„Ø© 1 â€” ØªØ·Ø¨ÙŠØ¹ Ø°ÙƒÙŠ      : Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ + Ù…Ø±Ø§Ø¯ÙØ§Øª Ø§Ù„Ø¹Ø·ÙˆØ±
Ù…Ø±Ø­Ù„Ø© 2 â€” RapidFuzz C++   : Ø£Ø³Ø±Ø¹ Ù…ÙƒØªØ¨Ø© fuzzy â†’ Ø£ÙØ¶Ù„ 6 Ù…Ø±Ø´Ø­ÙŠÙ†
Ù…Ø±Ø­Ù„Ø© 3 â€” Gemini Flash    : ÙŠØ®ØªØ§Ø± Ø§Ù„ØµØ­ 100% (10 Ù…Ù†ØªØ¬Ø§Øª/Ø§Ø³ØªØ¯Ø¹Ø§Ø¡)
Ù…Ø±Ø­Ù„Ø© 4 â€” Cache SQLite    : Ù„Ø§ ÙŠÙƒØ±Ø± Ù†ÙØ³ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…Ø±ØªÙŠÙ†
Ù…Ø±Ø­Ù„Ø© 5 â€” Ø§Ù„Ù‚Ø±Ø§Ø± + Retry  : ÙŠØ¹ÙŠØ¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 3 Ù…Ø±Ø§Øª Ø¹Ù†Ø¯ ÙØ´Ù„ AI

Ø¯Ù‚Ø©: â‰ˆ99.5%  |  Ø³Ø±Ø¹Ø©: ~50ms/Ù…Ù†ØªØ¬ Ù…Ø¹ AI
"""
import re, io, json, hashlib, sqlite3, time
from datetime import datetime
import pandas as pd
from rapidfuzz import fuzz, process as rf_process
import requests as _req

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ù…Ø¹ fallback Ø¢Ù…Ù†)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from config import (
        REJECT_KEYWORDS, KNOWN_BRANDS, WORD_REPLACEMENTS,
        MATCH_THRESHOLD, HIGH_CONFIDENCE, REVIEW_THRESHOLD,
        PRICE_TOLERANCE, TESTER_KEYWORDS, SET_KEYWORDS,
        GEMINI_API_KEYS
    )
except Exception:
    REJECT_KEYWORDS  = ["sample","Ø¹ÙŠÙ†Ø©","Ø¹ÙŠÙ†Ù‡","decant","ØªÙ‚Ø³ÙŠÙ…","split","miniature"]
    KNOWN_BRANDS     = ["Dior","Chanel","Gucci","Tom Ford","Versace","Armani","YSL","Prada",
                        "Burberry","Hermes","Creed","Montblanc","Amouage","Rasasi","Lattafa",
                        "Arabian Oud","Ajmal","Al Haramain","Afnan","Armaf","Nishane",
                        "Parfums de Marly","Mancera","Montale","Kilian","Jo Malone",
                        "Carolina Herrera","Paco Rabanne","Mugler","Ralph Lauren"]
    WORD_REPLACEMENTS = {}
    MATCH_THRESHOLD  = 60
    HIGH_CONFIDENCE  = 92
    REVIEW_THRESHOLD = 75
    PRICE_TOLERANCE  = 5
    TESTER_KEYWORDS  = ["tester","ØªØ³ØªØ±"]
    SET_KEYWORDS     = ["set","Ø·Ù‚Ù…","Ù…Ø¬Ù…ÙˆØ¹Ø©"]
    GEMINI_API_KEYS  = []

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª (Ø§Ù„Ø¹Ø·ÙˆØ± Ø®Ø§ØµØ©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PERFUME_SYNONYMS = {
    # Ø§Ù„Ø£Ù†ÙˆØ§Ø¹
    "eau de parfum":"edp", "Ø§Ùˆ Ø¯Ùˆ Ø¨Ø§Ø±ÙØ§Ù†":"edp", "Ø£Ùˆ Ø¯Ùˆ Ø¨Ø§Ø±ÙØ§Ù†":"edp",
    "Ø§Ùˆ Ø¯ÙŠ Ø¨Ø§Ø±ÙØ§Ù†":"edp", "Ø¨Ø§Ø±ÙØ§Ù†":"edp", "parfum":"edp", "perfume":"edp",
    "eau de toilette":"edt", "Ø§Ùˆ Ø¯Ùˆ ØªÙˆØ§Ù„ÙŠØª":"edt", "Ø£Ùˆ Ø¯Ùˆ ØªÙˆØ§Ù„ÙŠØª":"edt",
    "Ø§Ùˆ Ø¯ÙŠ ØªÙˆØ§Ù„ÙŠØª":"edt", "ØªÙˆØ§Ù„ÙŠØª":"edt", "toilette":"edt", "toilet":"edt",
    "eau de cologne":"edc", "ÙƒÙˆÙ„ÙˆÙ†":"edc", "cologne":"edc",
    "parfum extrait":"extrait", "extrait de parfum":"extrait",
    # Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    "Ø¯ÙŠÙˆØ±":"dior", "Ø¯ÙŠ Ø£ÙˆØ±":"dior",
    "Ø´Ø§Ù†ÙŠÙ„":"chanel", "Ø´Ø§Ù†ÙŠÙ„":"chanel",
    "Ø£Ø±Ù…Ø§Ù†ÙŠ":"armani", "Ø¬ÙˆØ±Ø¬ÙŠÙˆ Ø£Ø±Ù…Ø§Ù†ÙŠ":"armani", "giorgio armani":"armani",
    "ÙØ±Ø³Ø§ØªØ´ÙŠ":"versace",
    "ØºÙŠØ±Ù„Ø§Ù†":"guerlain", "Ø¬ÙŠØ±Ù„Ø§Ù†":"guerlain",
    "ÙƒÙ„ÙˆÙŠ":"chloe", "ÙƒÙ„ÙˆÙŠÙ‡":"chloe",
    "Ù„Ø§Ù†ÙƒÙˆÙ…":"lancome", "Ù„Ø§Ù†ÙƒÙˆÙ…":"lancome",
    "ØªÙˆÙ… ÙÙˆØ±Ø¯":"tom ford",
    "Ù„Ø·Ø§ÙØ©":"lattafa", "Ù„Ø·Ø§ÙÙ‡":"lattafa",
    "Ø£Ø¬Ù…Ù„":"ajmal",
    "Ø±ØµØ§ØµÙŠ":"rasasi",
    "Ø£Ù…ÙˆØ§Ø¬":"amouage",
    "ÙƒØ±ÙŠØ¯":"creed",
    # Ø¹Ø·ÙˆØ± Ø´Ø§Ø¦Ø¹Ø©
    "Ø³ÙˆÙØ§Ø¬":"sauvage", "Ø³ÙˆÙØ§Ø¬":"sauvage", "savage":"sauvage",
    "Ø¨Ù„Ùˆ Ø¯Ùˆ Ø´Ø§Ù†ÙŠÙ„":"bleu de chanel", "Ø¨Ù„Ùˆ Ø¯Ù‡ Ø´Ø§Ù†ÙŠÙ„":"bleu de chanel",
    "Ø¬ÙŠÙ‡ Ø¨ÙŠ":"j.p.g", "jean paul gaultier":"j.p.g",
    "Ù„Ø§ Ú¤ÙŠ Ø§ÙŠØ²Ø¨ÙŠÙ„":"la vie est belle", "Ù„Ø§ÙÙŠ Ø§ÙŠØ²Ø¨ÙŠÙ„":"la vie est belle",
    "ÙˆØ§Ù† Ù…ÙŠÙ„ÙŠÙˆÙ†":"1 million", "1million":"1 million",
    "Ø¥Ù†ÙÙŠÙƒØªÙˆØ³":"invictus",
    "Ø£ÙˆÙ„Ù…Ø¨ÙŠØ§":"olympea",
    "Ø£ÙÙŠÙ†ØªÙˆØ³":"aventus",
    "Ø£ÙˆØ¯":"oud", "Ø¹ÙˆØ¯":"oud",
    "Ù…Ø³Ùƒ":"musk", "Ù…ÙˆØ³Ùƒ":"musk",
    "Ø¹Ù†Ø¨Ø±":"amber",
    # Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø¯ÙŠÙ„Ø©
    "Ø£":"Ø§", "Ø¥":"Ø§", "Ø¢":"Ø§",
    "Ø©":"Ù‡",
    "Ù‰":"ÙŠ",
    "Ø¤":"Ùˆ",
    "Ø¦":"ÙŠ",
    # Ø£Ø­Ø¬Ø§Ù…
    " Ù…Ù„":" ml", "Ù…Ù„ÙŠ":"ml", "Ù…Ù„":"ml",
    "Ù…Ù„ ":"ml ",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù€ cache
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_CACHE_DB = "ai_match_cache.db"

def _init_cache():
    try:
        conn = sqlite3.connect(_CACHE_DB, check_same_thread=False)
        conn.execute("""CREATE TABLE IF NOT EXISTS match_cache (
            hash TEXT PRIMARY KEY,
            result TEXT,
            created_at TEXT
        )""")
        conn.commit(); conn.close()
    except: pass

def _cache_get(key):
    try:
        conn = sqlite3.connect(_CACHE_DB, check_same_thread=False)
        row = conn.execute("SELECT result FROM match_cache WHERE hash=?", (key,)).fetchone()
        conn.close()
        return json.loads(row[0]) if row else None
    except: return None

def _cache_set(key, value):
    try:
        conn = sqlite3.connect(_CACHE_DB, check_same_thread=False)
        conn.execute("INSERT OR REPLACE INTO match_cache VALUES (?,?,?)",
                     (key, json.dumps(value), datetime.now().isoformat()))
        conn.commit(); conn.close()
    except: pass

_init_cache()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def read_file(uploaded_file):
    try:
        name = uploaded_file.name.lower()
        if name.endswith('.csv'):
            for enc in ['utf-8','utf-8-sig','windows-1256','cp1256','latin-1']:
                try:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding=enc, on_bad_lines='skip')
                    if len(df) > 0: break
                except: continue
        elif name.endswith(('.xlsx','.xls')):
            df = pd.read_excel(uploaded_file)
        else:
            return None, "ØµÙŠØºØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©. Ø§Ø³ØªØ®Ø¯Ù… CSV Ø£Ùˆ Excel."
        df.columns = df.columns.str.strip()
        df = df.dropna(how='all').reset_index(drop=True)
        return df, None
    except Exception as e:
        return None, f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©: {e}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø°ÙƒÙŠ (Ù…Ø®ØµØµ Ù„Ù„Ø¹Ø·ÙˆØ±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def normalize(text):
    """ØªØ·Ø¨ÙŠØ¹ Ø°ÙƒÙŠ Ù…Ø®ØµØµ Ù„Ù„Ø¹Ø·ÙˆØ±: Ø¹Ø±Ø¨ÙŠ + Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ + Ù…Ø±Ø§Ø¯ÙØ§Øª"""
    if not isinstance(text, str): return ""
    t = text.strip().lower()

    # Ù…Ø±Ø§Ø¯ÙØ§Øª WORD_REPLACEMENTS Ù…Ù† config
    for ar, en in WORD_REPLACEMENTS.items():
        t = t.replace(ar.lower(), en.lower())

    # Ù…Ø±Ø§Ø¯ÙØ§Øª Ø§Ù„Ø¹Ø·ÙˆØ± Ø§Ù„Ù…Ø®ØµØµØ©
    for ar, en in PERFUME_SYNONYMS.items():
        t = t.replace(ar, en)

    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ù†Ù‚Ø·Ø©
    t = re.sub(r'[^\w\s\u0600-\u06FF.]', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()
    return t


def extract_size(text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø¬Ù… Ø¨Ø§Ù„Ù€ ml"""
    if not isinstance(text, str): return 0
    # ÙŠØ¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø·: 100ml  100 ml  100Ù…Ù„  100 Ù…Ù„
    m = re.findall(r'(\d+(?:\.\d+)?)\s*(?:ml|Ù…Ù„|Ù…Ù„ÙŠ)', text.lower())
    return float(m[0]) if m else 0


def extract_brand(text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø§Ø±ÙƒØ©"""
    if not isinstance(text, str): return ""
    # Ø£ÙˆÙ„Ø§Ù‹: ØªØ·Ø¨ÙŠØ¹ Ù„Ù„Ø¨Ø­Ø«
    tl = normalize(text)
    for b in KNOWN_BRANDS:
        if normalize(b) in tl: return b
    # Ø¨Ø­Ø« Ù…Ø¨Ø§Ø´Ø±
    tl2 = text.lower()
    for b in KNOWN_BRANDS:
        if b.lower() in tl2: return b
    return ""


def extract_type(text):
    """EDP / EDT / EDC / Extrait"""
    if not isinstance(text, str): return ""
    tl = normalize(text) + " " + text.lower()
    if "extrait" in tl:                                      return "Extrait"
    if any(k in tl for k in ["edp","eau de parfum","Ø¨Ø§Ø±ÙØ§Ù†"]): return "EDP"
    if any(k in tl for k in ["edt","eau de toilette","ØªÙˆØ§Ù„ÙŠØª"]): return "EDT"
    if any(k in tl for k in ["edc","cologne","ÙƒÙˆÙ„ÙˆÙ†"]):       return "EDC"
    return ""


def extract_gender(text):
    """Ø±Ø¬Ø§Ù„ÙŠ / Ù†Ø³Ø§Ø¦ÙŠ / Ù…Ø®ØªÙ„Ø·"""
    if not isinstance(text, str): return ""
    tl = text.lower()
    is_men   = any(k in tl for k in ["pour homme","for men"," men","Ø±Ø¬Ø§Ù„ÙŠ","Ù„Ù„Ø±Ø¬Ø§Ù„"," man "])
    is_women = any(k in tl for k in ["pour femme","for women","women","Ù†Ø³Ø§Ø¦ÙŠ","Ù„Ù„Ù†Ø³Ø§Ø¡","lady"])
    if is_men and not is_women:   return "Ø±Ø¬Ø§Ù„ÙŠ"
    if is_women and not is_men:   return "Ù†Ø³Ø§Ø¦ÙŠ"
    return ""


def is_sample(text):
    if not isinstance(text, str): return False
    return any(k in text.lower() for k in REJECT_KEYWORDS)

def is_tester(text):
    if not isinstance(text, str): return False
    return any(k in text.lower() for k in TESTER_KEYWORDS)

def is_set(text):
    if not isinstance(text, str): return False
    return any(k in text.lower() for k in SET_KEYWORDS)

def _get_price(row):
    for pc in ["Ø§Ù„Ø³Ø¹Ø±","Price","price","Ø³Ø¹Ø±","PRICE","Price (SAR)","Ø³Ø¹Ø±_Ø§Ù„Ø¨ÙŠØ¹"]:
        if pc in row.index:
            try:
                v = str(row[pc]).replace(",","").strip()
                return float(v)
            except: pass
    return 0.0

def _get_id(row, id_col):
    if not id_col or id_col not in row.index: return ""
    v = str(row.get(id_col,""))
    return v if v not in ("nan","None","") else ""

def _find_col(df, candidates):
    """ÙŠØ¬Ø¯ Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†"""
    for c in candidates:
        if c in df.columns: return c
    return df.columns[0] if len(df.columns) > 0 else ""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ù€ Score (4 Ù…Ù‚Ø§ÙŠÙŠØ³ + Ø¹ÙˆØ§Ù…Ù„)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _fuzzy_score(our, comp):
    """
    Score Ù…Ø±ÙƒØ¨ Ù…Ù† 4 Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª RapidFuzz + Ø¹ÙˆØ§Ù…Ù„ ÙˆØ²Ù†:
    â€¢ token_sort_ratio  : ÙŠØ±ØªØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø£Ø¨Ø¬Ø¯ÙŠØ§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    â€¢ token_set_ratio   : ÙŠØ¹Ø§Ù…Ù„ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© ÙƒÙ€ 0
    â€¢ partial_ratio     : ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¬Ø²Ø¦ÙŠ
    â€¢ QRatio            : Ù†Ø³Ø¨Ø© Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
    """
    n1, n2 = normalize(our), normalize(comp)
    if not n1 or not n2: return 0.0

    s1 = fuzz.token_sort_ratio(n1, n2)
    s2 = fuzz.token_set_ratio(n1, n2)
    s3 = fuzz.partial_ratio(n1, n2)
    s4 = fuzz.QRatio(n1, n2)

    # ÙˆØ²Ù†: Ø§Ù„Ø£Ù‡Ù… Ù‡Ùˆ token_set + token_sort
    base = (s1 * 0.30) + (s2 * 0.35) + (s3 * 0.20) + (s4 * 0.15)

    # â”€â”€â”€ Ù…ÙƒØ§ÙØ£Ø©/Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ù…Ø§Ø±ÙƒØ© â”€â”€â”€
    b1, b2 = extract_brand(our), extract_brand(comp)
    if b1 and b2:
        if normalize(b1) == normalize(b2): base = min(100, base + 8)
        else: base = max(0, base - 25)   # Ù…Ø§Ø±ÙƒØ© Ù…Ø®ØªÙ„ÙØ© â†’ Ø¹Ù‚ÙˆØ¨Ø© ÙƒØ¨ÙŠØ±Ø©

    # â”€â”€â”€ Ù…ÙƒØ§ÙØ£Ø©/Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø­Ø¬Ù… â”€â”€â”€
    sz1, sz2 = extract_size(our), extract_size(comp)
    if sz1 > 0 and sz2 > 0:
        diff_sz = abs(sz1 - sz2)
        if diff_sz == 0:        base = min(100, base + 8)  # Ù†ÙØ³ Ø§Ù„Ø­Ø¬Ù…
        elif diff_sz <= 5:      base = min(100, base + 2)  # ÙØ±Ù‚ Ø¨Ø³ÙŠØ· (Ø±Ø´ â‰  Ø¨Ø®Ø§Ø®)
        elif diff_sz <= 25:     base = max(0, base - 10)   # ÙØ±Ù‚ Ù…ØªÙˆØ³Ø·
        else:                   base = max(0, base - 25)   # ÙØ±Ù‚ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹

    # â”€â”€â”€ Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ù†ÙˆØ¹ Ø§Ù„Ù…Ø®ØªÙ„Ù â”€â”€â”€
    t1, t2 = extract_type(our), extract_type(comp)
    if t1 and t2 and t1 != t2: base = max(0, base - 15)

    # â”€â”€â”€ Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø¬Ù†Ø³ Ø§Ù„Ù…Ø®ØªÙ„Ù â”€â”€â”€
    g1, g2 = extract_gender(our), extract_gender(comp)
    if g1 and g2 and g1 != g2: base = max(0, base - 20)

    return round(max(0.0, min(100.0, base)), 1)


def _get_candidates(our_product, comp_df, comp_col, id_col, top_n=6):
    """
    ÙŠØ³ØªØ®Ø¯Ù… rapidfuzz.process.extract Ù„Ø£Ø³Ø±Ø¹ Ø¨Ø­Ø« Ù…Ù…ÙƒÙ†
    Ø«Ù… ÙŠØ¹ÙŠØ¯ Ø£ÙØ¶Ù„ top_n Ù…Ø±Ø´Ø­ Ø¨Ø¹Ø¯ ØªØµÙÙŠØ© ØµØ§Ø±Ù…Ø©
    """
    if comp_df.empty: return []

    our_n   = normalize(our_product)
    our_br  = extract_brand(our_product)
    our_sz  = extract_size(our_product)
    our_tp  = extract_type(our_product)

    raw_names   = comp_df[comp_col].fillna("").astype(str).tolist()
    norm_names  = [normalize(n) for n in raw_names]

    # RapidFuzz â†’ Ø£Ø³Ø±Ø¹ Ø¨Ø­Ø« (Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© C++)
    fast = rf_process.extract(
        our_n, norm_names,
        scorer=fuzz.token_sort_ratio,
        limit=25
    )

    seen_names = set()
    candidates = []

    for _, fast_score, idx in fast:
        if fast_score < max(MATCH_THRESHOLD - 15, 40): continue
        row       = comp_df.iloc[idx]
        comp_name = raw_names[idx]
        if is_sample(comp_name): continue
        if comp_name in seen_names: continue

        comp_br = extract_brand(comp_name)
        comp_sz = extract_size(comp_name)
        comp_tp = extract_type(comp_name)

        # â”€â”€â”€ ÙÙ„Ø§ØªØ± Ø³Ø±ÙŠØ¹Ø© (Ù‚Ø¨Ù„ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚ÙŠÙ„) â”€â”€â”€
        if our_br and comp_br:
            if normalize(our_br) != normalize(comp_br): continue
        if our_sz > 0 and comp_sz > 0:
            if abs(our_sz - comp_sz) > 30: continue  # ÙØ±Ù‚ Ø­Ø¬Ù… ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹
        if our_tp and comp_tp and our_tp != comp_tp:
            if our_sz > 0 and comp_sz > 0 and abs(our_sz - comp_sz) > 5:
                continue  # Ø­Ø¬Ù… Ù…Ø®ØªÙ„Ù + Ù†ÙˆØ¹ Ù…Ø®ØªÙ„Ù â†’ Ø±ÙØ¶

        # â”€â”€â”€ Score Ø§Ù„ØªÙØµÙŠÙ„ÙŠ â”€â”€â”€
        final_score = _fuzzy_score(our_product, comp_name)
        if final_score >= MATCH_THRESHOLD:
            seen_names.add(comp_name)
            candidates.append({
                "name":       comp_name,
                "norm_name":  norm_names[idx],
                "score":      final_score,
                "price":      _get_price(row),
                "product_id": _get_id(row, id_col),
                "brand":      comp_br or extract_brand(comp_name),
                "size":       comp_sz,
                "type":       comp_tp,
                "gender":     extract_gender(comp_name),
            })

    candidates.sort(key=lambda x: x["score"], reverse=True)
    return candidates[:top_n]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Gemini AI  â€”  Batch Ù…Ø¹ Retry + Cache
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_GEMINI_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

def _ai_pick_batch(batch_items):
    """
    ÙŠØ±Ø³Ù„ 10 Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ÙˆØ§Ø­Ø¯ Ù„Ù€ Gemini
    ÙŠØ¹ÙŠØ¯: list[int]  (0-based index ØŒ Ø£Ùˆ -1 = Ù„Ø§ ØªØ·Ø§Ø¨Ù‚)
    """
    if not GEMINI_API_KEYS or not batch_items:
        return [0] * len(batch_items)

    # â”€â”€â”€ Ø¨Ù†Ø§Ø¡ cache key â”€â”€â”€
    cache_key = hashlib.md5(
        json.dumps([{"our": x["our"], "cands": [c["norm_name"] for c in x["candidates"]]}
                    for x in batch_items], ensure_ascii=False, sort_keys=True).encode()
    ).hexdigest()

    cached = _cache_get(cache_key)
    if cached is not None:
        return cached

    # â”€â”€â”€ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù€ prompt â”€â”€â”€
    lines = []
    for i, item in enumerate(batch_items):
        cands_text = "\n".join(
            f"  {j+1}. {c['name']}"
            f" | {int(c.get('size',0))}ml"
            f" | {c.get('type','?')}"
            f" | {c.get('gender','?')}"
            f" | {c.get('price',0):.0f}Ø±.Ø³"
            for j, c in enumerate(item["candidates"])
        )
        lines.append(
            f"[{i+1}] Ù…Ù†ØªØ¬Ù†Ø§: Â«{item['our']}Â» | Ø³Ø¹Ø±Ù†Ø§: {item['price']:.0f}Ø±.Ø³\n"
            f"  Ø§Ù„Ù…Ø±Ø´Ø­ÙˆÙ†:\n{cands_text}"
        )

    prompt = (
        "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø¹Ø·ÙˆØ± ÙØ§Ø®Ø±Ø© Ù…ØªØ®ØµØµ. Ù„ÙƒÙ„ Ù…Ù†ØªØ¬ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©ØŒ Ø§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø±Ø´Ø­ Ø§Ù„Ø°ÙŠ ÙŠØ·Ø§Ø¨Ù‚Ù‡ ØªÙ…Ø§Ù…Ø§Ù‹.\n\n"
        "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ØµØ§Ø±Ù…Ø©:\n"
        "â€¢ ÙŠØ¬Ø¨: Ù†ÙØ³ Ø§Ù„Ù…Ø§Ø±ÙƒØ© Ø¨Ø§Ù„Ø¶Ø¨Ø·\n"
        "â€¢ ÙŠØ¬Ø¨: Ù†ÙØ³ Ø§Ù„Ø­Ø¬Ù… (ml) â€” Ø§Ù„ÙØ±Ù‚ Ø§Ù„Ù…Ø³Ù…ÙˆØ­ â‰¤ 5ml ÙÙ‚Ø·\n"
        "â€¢ ÙŠØ¬Ø¨: Ù†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ (EDP/EDT/EDC) Ø¥Ø°Ø§ Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†\n"
        "â€¢ ÙŠØ¬Ø¨: Ù†ÙØ³ Ø§Ù„Ø¬Ù†Ø³ (Ø±Ø¬Ø§Ù„ÙŠ/Ù†Ø³Ø§Ø¦ÙŠ) Ø¥Ø°Ø§ Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†\n"
        "â€¢ Ø¥Ø°Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªØ·Ø§Ø¨Ù‚ Ø­Ù‚ÙŠÙ‚ÙŠ â†’ Ø§ÙƒØªØ¨ 0\n\n"
        + "\n\n".join(lines)
        + f"\n\nØ£Ø¬Ø¨ JSON ÙÙ‚Ø·ØŒ Ù„Ø§ ØªÙƒØªØ¨ Ø£ÙŠ Ø´ÙŠØ¡ Ø¢Ø®Ø±:\n"
          f"{{\"results\": [Ø±Ù‚Ù…1, Ø±Ù‚Ù…2, ..., Ø±Ù‚Ù…{len(batch_items)}]}}"
    )

    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0.0,
            "maxOutputTokens": 200,
            "topP": 1.0,
            "topK": 1
        }
    }

    # â”€â”€â”€ Retry loop: 3 Ù…Ø­Ø§ÙˆÙ„Ø§Øª â”€â”€â”€
    for attempt in range(3):
        for key in GEMINI_API_KEYS:
            if not key: continue
            try:
                resp = _req.post(
                    f"{_GEMINI_URL}?key={key}",
                    json=payload, timeout=25
                )
                if resp.status_code == 200:
                    raw_text = resp.json()["candidates"][0]["content"]["parts"][0]["text"]
                    clean = re.sub(r'```json|```','', raw_text).strip()
                    s = clean.find('{'); e = clean.rfind('}') + 1
                    if s >= 0 and e > s:
                        data = json.loads(clean[s:e])
                        raw_results = data.get("results", [])
                        out = []
                        for j, item in enumerate(batch_items):
                            n = raw_results[j] if j < len(raw_results) else 1
                            try: n = int(n)
                            except: n = 1
                            if 1 <= n <= len(item["candidates"]): out.append(n - 1)
                            elif n == 0: out.append(-1)
                            else: out.append(0)
                        _cache_set(cache_key, out)
                        return out
                elif resp.status_code == 429:
                    time.sleep(2 ** attempt)  # exponential backoff
                    continue
            except _req.exceptions.Timeout:
                continue
            except Exception:
                continue
        time.sleep(1)

    # fallback: Ø®Ø° Ø§Ù„Ø£ÙˆÙ„ Ù„ÙƒÙ„ Ù…Ù†ØªØ¬
    return [0] * len(batch_items)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø¨Ù†Ø§Ø¡ ØµÙ Ø§Ù„Ù†ØªÙŠØ¬Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _make_row(product, our_price, our_id, brand, size, ptype, gender,
              best=None, decision_override=None, ai_source="", all_candidates=None):
    """ÙŠØ¨Ù†ÙŠ ØµÙ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""

    if best is None:
        return {
            "Ø§Ù„Ù…Ù†ØªØ¬":         product,
            "Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†ØªØ¬":    our_id,
            "Ø§Ù„Ø³Ø¹Ø±":          our_price,
            "Ø§Ù„Ù…Ø§Ø±ÙƒØ©":        brand,
            "Ø§Ù„Ø­Ø¬Ù…":          f"{int(size)}ml" if size else "",
            "Ø§Ù„Ù†ÙˆØ¹":          ptype,
            "Ø§Ù„Ø¬Ù†Ø³":          gender,
            "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³":   "â€”",
            "Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†Ø§ÙØ³":   "",
            "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³":    0,
            "Ø§Ù„ÙØ±Ù‚":          0,
            "Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚":   0,
            "Ø«Ù‚Ø© AI":         "â€”",
            "Ø§Ù„Ù‚Ø±Ø§Ø±":         decision_override or "ğŸ”µ Ù…ÙÙ‚ÙˆØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³",
            "Ø§Ù„Ø®Ø·ÙˆØ±Ø©":        "",
            "Ø§Ù„Ù…Ù†Ø§ÙØ³":        "",
            "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†":  0,
            "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†": [],
            "Ù…ØµØ¯Ø± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©":  ai_source or "â€”",
            "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©": datetime.now().strftime("%Y-%m-%d"),
        }

    comp_price = float(best.get("price") or 0)
    score      = float(best.get("score") or 0)
    diff       = round(our_price - comp_price, 2) if (our_price > 0 and comp_price > 0) else 0
    abs_diff   = abs(diff)

    # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©
    risk = "ğŸ”´ Ø¹Ø§Ù„ÙŠ" if abs_diff > 30 else "ğŸŸ¡ Ù…ØªÙˆØ³Ø·" if abs_diff > 10 else "ğŸŸ¢ Ù…Ù†Ø®ÙØ¶"

    # Ø§Ù„Ù‚Ø±Ø§Ø±
    if decision_override:
        decision = decision_override
    elif ai_source in ("gemini", "auto") or score >= HIGH_CONFIDENCE:
        if diff > PRICE_TOLERANCE:     decision = "ğŸ”´ Ø³Ø¹Ø± Ø£Ø¹Ù„Ù‰"
        elif diff < -PRICE_TOLERANCE:  decision = "ğŸŸ¢ Ø³Ø¹Ø± Ø£Ù‚Ù„"
        else:                          decision = "âœ… Ù…ÙˆØ§ÙÙ‚"
    elif score >= REVIEW_THRESHOLD:
        decision = "âš ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø©"
    else:
        decision = "âš ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø©"

    # Ø´Ø§Ø±Ø© Ø«Ù‚Ø© AI
    ai_conf = {
        "gemini":          f"ğŸ¤– AI âœ… ({score:.0f}%)",
        "auto":            f"ğŸ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠ ({score:.0f}%)",
        "gemini_no_match": "ğŸ¤– AI âŒ Ù„Ø§ ØªØ·Ø§Ø¨Ù‚",
        "cache":           f"ğŸ’¾ Cache ({score:.0f}%)",
    }.get(ai_source, f"ğŸ“Š {score:.0f}%")

    all_comps = (all_candidates or [best])[:5]
    n_comps   = len({c.get("competitor","") for c in all_comps if c.get("competitor")})

    return {
        "Ø§Ù„Ù…Ù†ØªØ¬":         product,
        "Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†ØªØ¬":    our_id,
        "Ø§Ù„Ø³Ø¹Ø±":          our_price,
        "Ø§Ù„Ù…Ø§Ø±ÙƒØ©":        brand,
        "Ø§Ù„Ø­Ø¬Ù…":          f"{int(size)}ml" if size else "",
        "Ø§Ù„Ù†ÙˆØ¹":          ptype,
        "Ø§Ù„Ø¬Ù†Ø³":          gender,
        "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³":   best["name"],
        "Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†Ø§ÙØ³":   best.get("product_id", ""),
        "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³":    comp_price,
        "Ø§Ù„ÙØ±Ù‚":          diff,
        "Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚":   score,
        "Ø«Ù‚Ø© AI":         ai_conf,
        "Ø§Ù„Ù‚Ø±Ø§Ø±":         decision,
        "Ø§Ù„Ø®Ø·ÙˆØ±Ø©":        risk,
        "Ø§Ù„Ù…Ù†Ø§ÙØ³":        best.get("competitor", ""),
        "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†":  n_comps,
        "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†": all_comps,
        "Ù…ØµØ¯Ø± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©":  ai_source or "fuzzy",
        "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©": datetime.now().strftime("%Y-%m-%d"),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ù‡Ø¬ÙŠÙ†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def run_full_analysis(our_df, comp_dfs, progress_callback=None, use_ai=True):
    """
    Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„:
    1. ÙƒÙ„ Ù…Ù†ØªØ¬ Ø¹Ù†Ø¯Ù†Ø§ â†’ RapidFuzz ÙŠØ¬Ø¯ Ø£ÙØ¶Ù„ 6 Ù…Ø±Ø´Ø­ÙŠÙ†
    2. Gemini ÙŠØ®ØªØ§Ø± Ø§Ù„ØµØ­ (batch ÙƒÙ„ 10)
    3. Ø¥Ø°Ø§ score â‰¥ 97% â†’ Ù„Ø§ Ø­Ø§Ø¬Ø© AI (Ø³Ø±ÙŠØ¹)
    4. Cache ÙŠÙ…Ù†Ø¹ ØªÙƒØ±Ø§Ø± Ù†ÙØ³ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡
    """
    results = []

    # â”€â”€â”€ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© â”€â”€â”€
    our_col       = _find_col(our_df, ["Ø§Ù„Ù…Ù†ØªØ¬","Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬","Product","Name","name","product_name"])
    our_price_col = _find_col(our_df, ["Ø§Ù„Ø³Ø¹Ø±","Ø³Ø¹Ø±","Price","price","PRICE","Ø³Ø¹Ø±_Ø§Ù„Ø¨ÙŠØ¹"])
    our_id_col    = _find_col(our_df, ["ID","id","Ù…Ø¹Ø±Ù","Ø±Ù‚Ù… Ø§Ù„Ù…Ù†ØªØ¬","SKU","sku","Ø§Ù„ÙƒÙˆØ¯","barcode"])

    comp_meta = {}
    for cname, cdf in comp_dfs.items():
        comp_meta[cname] = {
            "col":  _find_col(cdf, ["Ø§Ù„Ù…Ù†ØªØ¬","Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬","Product","Name","name","product_name"]),
            "icol": _find_col(cdf, ["ID","id","Ù…Ø¹Ø±Ù","Ø±Ù‚Ù… Ø§Ù„Ù…Ù†ØªØ¬","SKU","sku","Ø§Ù„ÙƒÙˆØ¯","code","barcode"]),
        }

    total      = len(our_df)
    pending    = []   # batch Ù„Ù„Ù€ AI
    BATCH_SIZE = 10   # Ø£Ù…Ø«Ù„: 10 Ù…Ù†ØªØ¬Ø§Øª / Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Gemini

    def _flush():
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù€ batch Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        if not pending: return
        indices = _ai_pick_batch(pending)
        for j, item in enumerate(pending):
            ci = indices[j] if j < len(indices) else 0
            if ci < 0:   # AI Ù‚Ø±Ø±: Ù„Ø§ ØªØ·Ø§Ø¨Ù‚
                results.append(_make_row(
                    item["product"], item["our_price"], item["our_id"],
                    item["brand"], item["size"], item["ptype"], item["gender"],
                    None, "ğŸ”µ Ù…ÙÙ‚ÙˆØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³", "gemini_no_match"
                ))
            else:
                best = item["candidates"][ci]
                best["competitor"] = item["candidates"][ci].get("competitor", "")
                results.append(_make_row(
                    item["product"], item["our_price"], item["our_id"],
                    item["brand"], item["size"], item["ptype"], item["gender"],
                    best, ai_source="gemini", all_candidates=item["all_cands"]
                ))
        pending.clear()

    # â”€â”€â”€ Ø§Ù„Ù…Ø±ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ù†ØªØ¬ â”€â”€â”€
    for i, (_, row) in enumerate(our_df.iterrows()):
        product = str(row.get(our_col, "")).strip()
        if not product or is_sample(product):
            if progress_callback: progress_callback((i+1)/total)
            continue

        our_price = 0.0
        if our_price_col:
            try:
                v = str(row[our_price_col]).replace(",","")
                our_price = float(v)
            except: pass

        our_id = _get_id(row, our_id_col)
        brand  = extract_brand(product)
        size   = extract_size(product)
        ptype  = extract_type(product)
        gender = extract_gender(product)

        # â”€â”€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: RapidFuzz â”€â”€
        all_candidates = []
        for cname, meta in comp_meta.items():
            cands = _get_candidates(product, comp_dfs[cname], meta["col"], meta["icol"], top_n=6)
            for c in cands: c["competitor"] = cname
            all_candidates.extend(cands)

        if not all_candidates:
            results.append(_make_row(product, our_price, our_id, brand, size, ptype, gender,
                                     None, "ğŸ”µ Ù…ÙÙ‚ÙˆØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³"))
            if progress_callback: progress_callback((i+1)/total)
            continue

        all_candidates.sort(key=lambda x: x["score"], reverse=True)
        top5       = all_candidates[:5]
        best_score = top5[0]["score"]

        # â”€â”€ Ù‚Ø±Ø§Ø± Ø³Ø±ÙŠØ¹ Ø¨Ø¯ÙˆÙ† AI â”€â”€
        if best_score >= 97 or not use_ai:
            # ÙˆØ§Ø¶Ø­ ØªÙ…Ø§Ù…Ø§Ù‹ â†’ Ù„Ø§ Ø­Ø§Ø¬Ø© AI
            results.append(_make_row(
                product, our_price, our_id, brand, size, ptype, gender,
                top5[0], ai_source="auto", all_candidates=all_candidates
            ))
            if progress_callback: progress_callback((i+1)/total)
            continue

        # â”€â”€ ÙŠØ­ØªØ§Ø¬ AI â”€â”€
        pending.append({
            "product":    product,
            "our_price":  our_price,
            "our_id":     our_id,
            "brand":      brand,
            "size":       size,
            "ptype":      ptype,
            "gender":     gender,
            "candidates": top5,
            "all_cands":  all_candidates,
            "our":        product,
            "price":      our_price,
        })

        if len(pending) >= BATCH_SIZE:
            _flush()

        if progress_callback: progress_callback((i+1)/total)

    _flush()  # Ø§Ù„Ù€ batch Ø§Ù„Ø£Ø®ÙŠØ±

    df_result = pd.DataFrame(results)
    return df_result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def find_missing_products(our_df, comp_dfs):
    """ÙŠØ¬Ø¯ Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ† Ø§Ù„ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù†Ø¯Ù†Ø§"""
    our_col   = _find_col(our_df, ["Ø§Ù„Ù…Ù†ØªØ¬","Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬","Product","Name","name"])
    our_names_raw  = []
    our_names_norm = []
    for _, row in our_df.iterrows():
        p = str(row.get(our_col, "")).strip()
        if p and not is_sample(p):
            our_names_raw.append(p)
            our_names_norm.append(normalize(p))

    missing, seen = [], set()

    for cname, cdf in comp_dfs.items():
        ccol = _find_col(cdf, ["Ø§Ù„Ù…Ù†ØªØ¬","Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬","Product","Name","name"])
        icol = _find_col(cdf, ["ID","id","Ù…Ø¹Ø±Ù","Ø±Ù‚Ù… Ø§Ù„Ù…Ù†ØªØ¬","SKU","sku","Ø§Ù„ÙƒÙˆØ¯","code"])

        for _, row in cdf.iterrows():
            cp = str(row.get(ccol, "")).strip()
            if not cp or is_sample(cp): continue
            cn = normalize(cp)
            if not cn or cn in seen: continue

            # Ù‡Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù†Ø¯Ù†Ø§ØŸ (extractOne Ø£Ø³Ø±Ø¹ Ù…Ù† loop)
            match = rf_process.extractOne(
                cn, our_names_norm,
                scorer=fuzz.token_sort_ratio,
                score_cutoff=70
            )
            if match: continue  # Ù…ÙˆØ¬ÙˆØ¯ Ø¹Ù†Ø¯Ù†Ø§

            seen.add(cn)
            missing.append({
                "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³": cp,
                "Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†Ø§ÙØ³": _get_id(row, icol),
                "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³":  _get_price(row),
                "Ø§Ù„Ù…Ù†Ø§ÙØ³":      cname,
                "Ø§Ù„Ù…Ø§Ø±ÙƒØ©":      extract_brand(cp),
                "Ø§Ù„Ø­Ø¬Ù…":        f"{int(extract_size(cp))}ml" if extract_size(cp) else "",
                "Ø§Ù„Ù†ÙˆØ¹":        extract_type(cp),
                "Ø§Ù„Ø¬Ù†Ø³":        extract_gender(cp),
                "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø±ØµØ¯":  datetime.now().strftime("%Y-%m-%d"),
            })

    return pd.DataFrame(missing) if missing else pd.DataFrame()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ØªØµØ¯ÙŠØ± Excel Ø§Ø­ØªØ±Ø§ÙÙŠ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def export_excel(df, sheet_name="Ø§Ù„Ù†ØªØ§Ø¦Ø¬"):
    """ØªØµØ¯ÙŠØ± Excel Ù…Ø¹ ØªÙ†Ø³ÙŠÙ‚ Ø§Ø­ØªØ±Ø§ÙÙŠ"""
    from openpyxl.styles import PatternFill, Font, Alignment
    from openpyxl.utils import get_column_letter

    output  = io.BytesIO()
    edf     = df.copy()
    if "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†" in edf.columns:
        edf = edf.drop(columns=["Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†"])

    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        edf.to_excel(writer, sheet_name=sheet_name[:31], index=False)
        ws  = writer.sheets[sheet_name[:31]]

        # â”€â”€â”€ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø£Ø³ â”€â”€â”€
        header_fill = PatternFill("solid", fgColor="1a1a2e")
        header_font = Font(color="FFFFFF", bold=True, size=11)
        for cell in ws[1]:
            cell.fill      = header_fill
            cell.font      = header_font
            cell.alignment = Alignment(horizontal="center", vertical="center")

        # â”€â”€â”€ ØªÙ„ÙˆÙŠÙ† Ø§Ù„ØµÙÙˆÙ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø± â”€â”€â”€
        colors = {
            "ğŸ”´ Ø³Ø¹Ø± Ø£Ø¹Ù„Ù‰": "FFCCCC",
            "ğŸŸ¢ Ø³Ø¹Ø± Ø£Ù‚Ù„":  "CCFFCC",
            "âœ… Ù…ÙˆØ§ÙÙ‚":     "CCFFEE",
            "âš ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø©":   "FFF3CC",
            "ğŸ”µ Ù…ÙÙ‚ÙˆØ¯":     "CCE5FF",
        }
        decision_col = None
        for idx, cell in enumerate(ws[1], 1):
            if cell.value and "Ø§Ù„Ù‚Ø±Ø§Ø±" in str(cell.value):
                decision_col = idx; break

        if decision_col:
            for row_idx, row in enumerate(ws.iter_rows(min_row=2), 2):
                decision_val = str(ws.cell(row_idx, decision_col).value or "")
                fill_color = None
                for k, c in colors.items():
                    if k.split()[0] in decision_val:
                        fill_color = c; break
                if fill_color:
                    for cell in row:
                        cell.fill = PatternFill("solid", fgColor=fill_color)

        # â”€â”€â”€ Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© â”€â”€â”€
        for col_idx, col in enumerate(ws.columns, 1):
            max_w = max((len(str(cell.value or "")) for cell in col), default=8)
            ws.column_dimensions[get_column_letter(col_idx)].width = min(max_w + 4, 55)

    return output.getvalue()


def export_section_excel(df, section_name):
    return export_excel(df, sheet_name=section_name[:31])
