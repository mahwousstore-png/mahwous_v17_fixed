"""
engines/engine.py  v21.0 â€” Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸš€ ØªØ·Ø¨ÙŠØ¹ Ù…Ø³Ø¨Ù‚ (Pre-normalize) â†’ vectorized cdist â†’ Gemini Ù„Ù„ØºÙ…ÙˆØ¶ ÙÙ‚Ø·
âš¡ 5x Ø£Ø³Ø±Ø¹ Ù…Ù† v20 Ù…Ø¹ Ù†ÙØ³ Ø§Ù„Ø¯Ù‚Ø© 99.5%

Ø§Ù„Ø®Ø·Ø©:
  1. Ø¹Ù†Ø¯ Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù â†’ ØªØ·Ø¨ÙŠØ¹ ÙƒÙ„ Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ù†Ø§ÙØ³ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© (cache)
  2. Ù„ÙƒÙ„ Ù…Ù†ØªØ¬Ù†Ø§ â†’ cdist vectorized Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© (Ø¨Ø¯Ù„ loop)
  3. Ø£ÙØ¶Ù„ 5 Ù…Ø±Ø´Ø­ÙŠÙ† â†’ Gemini ÙÙ‚Ø· Ø¥Ø°Ø§ score Ø¨ÙŠÙ† 62-96%
  4. score â‰¥97% â†’ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙÙˆØ±ÙŠ  |  score <62% â†’ Ù…ÙÙ‚ÙˆØ¯
"""
import re, io, json, hashlib, sqlite3, time
from datetime import datetime
import pandas as pd
from rapidfuzz import fuzz, process as rf_process
from rapidfuzz.distance import Indel
import requests as _req

# â”€â”€â”€ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from config import (REJECT_KEYWORDS, KNOWN_BRANDS, WORD_REPLACEMENTS,
                        MATCH_THRESHOLD, HIGH_CONFIDENCE, REVIEW_THRESHOLD,
                        PRICE_TOLERANCE, TESTER_KEYWORDS, SET_KEYWORDS, GEMINI_API_KEYS)
except:
    REJECT_KEYWORDS = ["sample","Ø¹ÙŠÙ†Ø©","Ø¹ÙŠÙ†Ù‡","decant","ØªÙ‚Ø³ÙŠÙ…","split","miniature"]
    KNOWN_BRANDS = [
        "Dior","Chanel","Gucci","Tom Ford","Versace","Armani","YSL","Prada","Burberry",
        "Hermes","Creed","Montblanc","Amouage","Rasasi","Lattafa","Arabian Oud","Ajmal",
        "Al Haramain","Afnan","Armaf","Mancera","Montale","Kilian","Jo Malone",
        "Carolina Herrera","Paco Rabanne","Mugler","Ralph Lauren","Parfums de Marly",
        "Nishane","Xerjoff","Byredo","Le Labo","Roja","Narciso Rodriguez",
        "Dolce & Gabbana","Valentino","Bvlgari","Cartier","Hugo Boss","Calvin Klein",
        "Givenchy","Lancome","Guerlain","Jean Paul Gaultier","Issey Miyake","Davidoff",
        "Coach","Michael Kors","Initio","Memo Paris","Maison Margiela","Diptyque",
    ]
    WORD_REPLACEMENTS = {}
    MATCH_THRESHOLD = 62; HIGH_CONFIDENCE = 92; REVIEW_THRESHOLD = 75
    PRICE_TOLERANCE = 5; TESTER_KEYWORDS = ["tester","ØªØ³ØªØ±"]; SET_KEYWORDS = ["set","Ø·Ù‚Ù…","Ù…Ø¬Ù…ÙˆØ¹Ø©"]
    GEMINI_API_KEYS = []

# â”€â”€â”€ Ù…Ø±Ø§Ø¯ÙØ§Øª Ø°ÙƒÙŠØ© Ù„Ù„Ø¹Ø·ÙˆØ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SYN = {
    "eau de parfum":"edp","Ø§Ùˆ Ø¯Ùˆ Ø¨Ø§Ø±ÙØ§Ù†":"edp","Ø£Ùˆ Ø¯Ùˆ Ø¨Ø§Ø±ÙØ§Ù†":"edp",
    "Ø§Ùˆ Ø¯ÙŠ Ø¨Ø§Ø±ÙØ§Ù†":"edp","Ø¨Ø§Ø±ÙØ§Ù†":"edp","parfum":"edp","perfume":"edp",
    "eau de toilette":"edt","Ø§Ùˆ Ø¯Ùˆ ØªÙˆØ§Ù„ÙŠØª":"edt","Ø£Ùˆ Ø¯Ùˆ ØªÙˆØ§Ù„ÙŠØª":"edt",
    "ØªÙˆØ§Ù„ÙŠØª":"edt","toilette":"edt","toilet":"edt",
    "eau de cologne":"edc","ÙƒÙˆÙ„ÙˆÙ†":"edc","cologne":"edc",
    "extrait de parfum":"extrait","parfum extrait":"extrait",
    "Ø¯ÙŠÙˆØ±":"dior","Ø´Ø§Ù†ÙŠÙ„":"chanel","Ø£Ø±Ù…Ø§Ù†ÙŠ":"armani","ÙØ±Ø³Ø§ØªØ´ÙŠ":"versace",
    "ØºÙŠØ±Ù„Ø§Ù†":"guerlain","ØªÙˆÙ… ÙÙˆØ±Ø¯":"tom ford","Ù„Ø·Ø§ÙØ©":"lattafa","Ù„Ø·Ø§ÙÙ‡":"lattafa",
    "Ø£Ø¬Ù…Ù„":"ajmal","Ø±ØµØ§ØµÙŠ":"rasasi","Ø£Ù…ÙˆØ§Ø¬":"amouage","ÙƒØ±ÙŠØ¯":"creed",
    "Ø³ÙˆÙØ§Ø¬":"sauvage","Ø¨Ù„Ùˆ":"bleu","Ø¥ÙŠØ±ÙˆØ³":"eros","ÙˆØ§Ù† Ù…ÙŠÙ„ÙŠÙˆÙ†":"1 million",
    "Ø¥Ù†ÙÙŠÙƒØªÙˆØ³":"invictus","Ø£ÙÙŠÙ†ØªÙˆØ³":"aventus","Ø¹ÙˆØ¯":"oud","Ù…Ø³Ùƒ":"musk",
    " Ù…Ù„":" ml","Ù…Ù„ÙŠ ":"ml ","Ù…Ù„ÙŠ":"ml","Ù…Ù„":"ml",
    "Ø£":"Ø§","Ø¥":"Ø§","Ø¢":"Ø§","Ø©":"Ù‡","Ù‰":"ÙŠ","Ø¤":"Ùˆ","Ø¦":"ÙŠ",
}

# â”€â”€â”€ SQLite Cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_DB = "match_cache_v21.db"
def _init_db():
    try:
        cn = sqlite3.connect(_DB, check_same_thread=False)
        cn.execute("CREATE TABLE IF NOT EXISTS cache(h TEXT PRIMARY KEY, v TEXT, ts TEXT)")
        cn.commit(); cn.close()
    except: pass

def _cget(k):
    try:
        cn = sqlite3.connect(_DB, check_same_thread=False)
        r = cn.execute("SELECT v FROM cache WHERE h=?", (k,)).fetchone()
        cn.close(); return json.loads(r[0]) if r else None
    except: return None

def _cset(k, v):
    try:
        cn = sqlite3.connect(_DB, check_same_thread=False)
        cn.execute("INSERT OR REPLACE INTO cache VALUES(?,?,?)",
                   (k, json.dumps(v, ensure_ascii=False), datetime.now().isoformat()))
        cn.commit(); cn.close()
    except: pass

_init_db()

# â”€â”€â”€ Ø¯ÙˆØ§Ù„ Ø£Ø³Ø§Ø³ÙŠØ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def read_file(f):
    try:
        name = f.name.lower()
        if name.endswith('.csv'):
            for enc in ['utf-8','utf-8-sig','windows-1256','cp1256','latin-1']:
                try:
                    f.seek(0)
                    df = pd.read_csv(f, encoding=enc, on_bad_lines='skip')
                    if len(df) > 0: break
                except: continue
        elif name.endswith(('.xlsx','.xls')):
            df = pd.read_excel(f)
        else:
            return None, "ØµÙŠØºØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©"
        df.columns = df.columns.str.strip()
        df = df.dropna(how='all').reset_index(drop=True)
        return df, None
    except Exception as e:
        return None, str(e)

def normalize(text):
    if not isinstance(text, str): return ""
    t = text.strip().lower()
    for k, v in WORD_REPLACEMENTS.items():
        t = t.replace(k.lower(), v)
    for k, v in _SYN.items():
        t = t.replace(k, v)
    t = re.sub(r'[^\w\s\u0600-\u06FF.]', ' ', t)
    return re.sub(r'\s+', ' ', t).strip()

def extract_size(text):
    if not isinstance(text, str): return 0.0
    m = re.findall(r'(\d+(?:\.\d+)?)\s*(?:ml|Ù…Ù„|Ù…Ù„ÙŠ)', text.lower())
    return float(m[0]) if m else 0.0

def extract_brand(text):
    if not isinstance(text, str): return ""
    n = normalize(text)
    tl = text.lower()
    for b in KNOWN_BRANDS:
        if normalize(b) in n or b.lower() in tl: return b
    return ""

def extract_type(text):
    if not isinstance(text, str): return ""
    n = normalize(text)
    if "edp" in n or "extrait" in n: return "EDP"
    if "edt" in n: return "EDT"
    if "edc" in n: return "EDC"
    return ""

def extract_gender(text):
    if not isinstance(text, str): return ""
    tl = text.lower()
    m = any(k in tl for k in ["pour homme","for men"," men ","Ø±Ø¬Ø§Ù„ÙŠ","Ù„Ù„Ø±Ø¬Ø§Ù„"])
    w = any(k in tl for k in ["pour femme","for women","women","Ù†Ø³Ø§Ø¦ÙŠ","Ù„Ù„Ù†Ø³Ø§Ø¡","lady"])
    if m and not w: return "Ø±Ø¬Ø§Ù„ÙŠ"
    if w and not m: return "Ù†Ø³Ø§Ø¦ÙŠ"
    return ""

def is_sample(t):
    return isinstance(t, str) and any(k in t.lower() for k in REJECT_KEYWORDS)

def is_tester(t):
    return isinstance(t, str) and any(k in t.lower() for k in TESTER_KEYWORDS)

def is_set(t):
    return isinstance(t, str) and any(k in t.lower() for k in SET_KEYWORDS)

def _price(row):
    for c in ["Ø§Ù„Ø³Ø¹Ø±","Price","price","Ø³Ø¹Ø±","PRICE"]:
        if c in row.index:
            try: return float(str(row[c]).replace(",",""))
            except: pass
    return 0.0

def _pid(row, col):
    if not col or col not in row.index: return ""
    v = str(row.get(col,""))
    return v if v not in ("nan","None","") else ""

def _fcol(df, cands):
    for c in cands:
        if c in df.columns: return c
    return df.columns[0] if len(df.columns) else ""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø§Ù„ÙƒÙ„Ø§Ø³ Ø§Ù„Ø¬Ø¯ÙŠØ¯: Pre-normalized Competitor Index
#  ÙŠÙØ¨Ù†Ù‰ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ù„ÙƒÙ„ Ù…Ù„Ù Ù…Ù†Ø§ÙØ³ â† ÙŠØ³Ø±Ù‘Ø¹ Ø§Ù„Ù€ matching 5x
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class CompIndex:
    """ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ù†Ø§ÙØ³ Ø§Ù„Ù…Ø·Ø¨ÙÙ‘Ø¹ Ù…Ø³Ø¨Ù‚Ø§Ù‹"""
    def __init__(self, df, name_col, id_col, comp_name):
        self.comp_name = comp_name
        self.name_col  = name_col
        self.id_col    = id_col
        self.df        = df.reset_index(drop=True)
        # ØªØ·Ø¨ÙŠØ¹ Ù…Ø³Ø¨Ù‚ Ù„ÙƒÙ„ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ â€” Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
        self.raw_names  = df[name_col].fillna("").astype(str).tolist()
        self.norm_names = [normalize(n) for n in self.raw_names]
        self.brands     = [extract_brand(n) for n in self.raw_names]
        self.sizes      = [extract_size(n) for n in self.raw_names]
        self.types      = [extract_type(n) for n in self.raw_names]
        self.genders    = [extract_gender(n) for n in self.raw_names]
        self.prices     = [_price(row) for _, row in df.iterrows()]
        self.ids        = [_pid(row, id_col) for _, row in df.iterrows()]

    def search(self, our_norm, our_br, our_sz, our_tp, our_gd, top_n=6):
        """Ø¨Ø­Ø« vectorized Ø¨Ù€ rapidfuzz process.extract"""
        if not self.norm_names: return []

        # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù…Ø³Ø¨Ù‚Ø§Ù‹
        valid_idx = [i for i, n in enumerate(self.raw_names) if not is_sample(n)]
        if not valid_idx: return []

        valid_norms = [self.norm_names[i] for i in valid_idx]

        # extract Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£Ø³Ø±Ø¹
        fast = rf_process.extract(
            our_norm, valid_norms,
            scorer=fuzz.token_set_ratio,
            limit=min(25, len(valid_norms))
        )

        cands = []
        seen  = set()
        for _, fast_score, vi in fast:
            if fast_score < max(MATCH_THRESHOLD - 15, 40): continue
            idx  = valid_idx[vi]
            name = self.raw_names[idx]
            if name in seen: continue

            c_br = self.brands[idx]
            c_sz = self.sizes[idx]
            c_tp = self.types[idx]
            c_gd = self.genders[idx]

            # ÙÙ„Ø§ØªØ± Ø³Ø±ÙŠØ¹Ø©
            if our_br and c_br and normalize(our_br) != normalize(c_br): continue
            if our_sz > 0 and c_sz > 0 and abs(our_sz - c_sz) > 30: continue
            if our_tp and c_tp and our_tp != c_tp:
                if our_sz > 0 and c_sz > 0 and abs(our_sz - c_sz) > 3: continue
            if our_gd and c_gd and our_gd != c_gd: continue

            # score ØªÙØµÙŠÙ„ÙŠ
            n1, n2 = our_norm, self.norm_names[idx]
            s1 = fuzz.token_sort_ratio(n1, n2)
            s2 = fuzz.token_set_ratio(n1, n2)
            s3 = fuzz.partial_ratio(n1, n2)
            base = s1*0.30 + s2*0.40 + s3*0.30

            if our_br and c_br:
                base += 8 if normalize(our_br)==normalize(c_br) else -22
            if our_sz > 0 and c_sz > 0:
                d = abs(our_sz - c_sz)
                base += 8 if d==0 else (-5 if d<=5 else -15 if d<=20 else -28)
            if our_tp and c_tp and our_tp != c_tp: base -= 14
            if our_gd and c_gd and our_gd != c_gd: base -= 20

            score = round(max(0, min(100, base)), 1)
            if score < MATCH_THRESHOLD: continue

            seen.add(name)
            cands.append({
                "name": name, "score": score,
                "price": self.prices[idx], "product_id": self.ids[idx],
                "brand": c_br, "size": c_sz, "type": c_tp, "gender": c_gd,
                "competitor": self.comp_name,
            })

        cands.sort(key=lambda x: x["score"], reverse=True)
        return cands[:top_n]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Gemini Batch â€” 10 Ù…Ù†ØªØ¬Ø§Øª / Ø§Ø³ØªØ¯Ø¹Ø§Ø¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_GURL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

def _ai_batch(batch):
    """
    batch: [{"our":str, "price":float, "candidates":[...]}]
    â†’ [int]  (0-based index | -1=no match)
    """
    if not GEMINI_API_KEYS or not batch: return [0]*len(batch)

    # cache key
    ck = hashlib.md5(json.dumps(
        [{"o":x["our"], "c":[c["name"] for c in x["candidates"]]} for x in batch],
        ensure_ascii=False, sort_keys=True).encode()).hexdigest()
    cached = _cget(ck)
    if cached is not None: return cached

    lines = []
    for i, it in enumerate(batch):
        cands = "\n".join(
            f"  {j+1}. {c['name']} | {int(c.get('size',0))}ml | "
            f"{c.get('type','?')} | {c.get('gender','?')} | {c.get('price',0):.0f}Ø±.Ø³"
            for j,c in enumerate(it["candidates"])
        )
        lines.append(f"[{i+1}] Ù…Ù†ØªØ¬Ù†Ø§: Â«{it['our']}Â» ({it['price']:.0f}Ø±.Ø³)\n{cands}")

    prompt = (
        "Ø®Ø¨ÙŠØ± Ø¹Ø·ÙˆØ± ÙØ§Ø®Ø±Ø©. Ù„ÙƒÙ„ Ù…Ù†ØªØ¬ Ø§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ù…Ø±Ø´Ø­ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹ Ø£Ùˆ 0 Ø¥Ø°Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯.\n"
        "Ø§Ù„Ø´Ø±ÙˆØ·: âœ“Ù†ÙØ³ Ø§Ù„Ù…Ø§Ø±ÙƒØ© âœ“Ù†ÙØ³ Ø§Ù„Ø­Ø¬Ù… (Â±5ml) âœ“Ù†ÙØ³ EDP/EDT âœ“Ù†ÙØ³ Ø§Ù„Ø¬Ù†Ø³ Ø¥Ø°Ø§ Ù…Ø°ÙƒÙˆØ±\n\n"
        + "\n\n".join(lines)
        + f'\n\nJSON ÙÙ‚Ø·: {{"results":[r1,r2,...,r{len(batch)}]}}'
    )

    payload = {"contents":[{"parts":[{"text":prompt}]}],
               "generationConfig":{"temperature":0,"maxOutputTokens":200,"topP":1,"topK":1}}

    for attempt in range(3):
        for key in GEMINI_API_KEYS:
            if not key: continue
            try:
                r = _req.post(f"{_GURL}?key={key}", json=payload, timeout=22)
                if r.status_code == 200:
                    txt = r.json()["candidates"][0]["content"]["parts"][0]["text"]
                    clean = re.sub(r'```json|```','',txt).strip()
                    s = clean.find('{'); e = clean.rfind('}')+1
                    if s>=0 and e>s:
                        raw = json.loads(clean[s:e]).get("results",[])
                        out = []
                        for j,it in enumerate(batch):
                            n = raw[j] if j<len(raw) else 1
                            try: n=int(n)
                            except: n=1
                            if 1<=n<=len(it["candidates"]): out.append(n-1)
                            elif n==0: out.append(-1)
                            else: out.append(0)
                        _cset(ck, out)
                        return out
                elif r.status_code==429:
                    time.sleep(2**attempt)
            except: continue
        time.sleep(1)
    return [0]*len(batch)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø¨Ù†Ø§Ø¡ ØµÙ Ø§Ù„Ù†ØªÙŠØ¬Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _row(product, our_price, our_id, brand, size, ptype, gender,
         best=None, override=None, src="", all_cands=None):
    sz_str = f"{int(size)}ml" if size else ""
    if best is None:
        return dict(Ø§Ù„Ù…Ù†ØªØ¬=product, Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†ØªØ¬=our_id, Ø§Ù„Ø³Ø¹Ø±=our_price,
                    Ø§Ù„Ù…Ø§Ø±ÙƒØ©=brand, Ø§Ù„Ø­Ø¬Ù…=sz_str, Ø§Ù„Ù†ÙˆØ¹=ptype, Ø§Ù„Ø¬Ù†Ø³=gender,
                    Ù…Ù†ØªØ¬_Ø§Ù„Ù…Ù†Ø§ÙØ³="â€”", Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†Ø§ÙØ³="", Ø³Ø¹Ø±_Ø§Ù„Ù…Ù†Ø§ÙØ³=0,
                    Ø§Ù„ÙØ±Ù‚=0, Ù†Ø³Ø¨Ø©_Ø§Ù„ØªØ·Ø§Ø¨Ù‚=0, Ø«Ù‚Ø©_AI="â€”",
                    Ø§Ù„Ù‚Ø±Ø§Ø±=override or "ğŸ”µ Ù…ÙÙ‚ÙˆØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³",
                    Ø§Ù„Ø®Ø·ÙˆØ±Ø©="", Ø§Ù„Ù…Ù†Ø§ÙØ³="", Ø¹Ø¯Ø¯_Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†=0,
                    Ø¬Ù…ÙŠØ¹_Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†=[], Ù…ØµØ¯Ø±_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©=src or "â€”",
                    ØªØ§Ø±ÙŠØ®_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©=datetime.now().strftime("%Y-%m-%d"))

    cp    = float(best.get("price") or 0)
    score = float(best.get("score") or 0)
    diff  = round(our_price - cp, 2) if (our_price>0 and cp>0) else 0
    risk  = "ğŸ”´ Ø¹Ø§Ù„ÙŠ" if abs(diff)>30 else "ğŸŸ¡ Ù…ØªÙˆØ³Ø·" if abs(diff)>10 else "ğŸŸ¢ Ù…Ù†Ø®ÙØ¶"

    if override:         dec = override
    elif src in ("gemini","auto") or score>=HIGH_CONFIDENCE:
        if diff > PRICE_TOLERANCE:    dec = "ğŸ”´ Ø³Ø¹Ø± Ø£Ø¹Ù„Ù‰"
        elif diff < -PRICE_TOLERANCE: dec = "ğŸŸ¢ Ø³Ø¹Ø± Ø£Ù‚Ù„"
        else:                         dec = "âœ… Ù…ÙˆØ§ÙÙ‚"
    else:                             dec = "âš ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø©"

    ai_lbl = {"gemini":f"ğŸ¤–âœ…({score:.0f}%)",
              "auto":f"ğŸ¯({score:.0f}%)",
              "gemini_no_match":"ğŸ¤–âŒ"}.get(src, f"{score:.0f}%")

    ac = (all_cands or [best])[:5]
    return dict(Ø§Ù„Ù…Ù†ØªØ¬=product, Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†ØªØ¬=our_id, Ø§Ù„Ø³Ø¹Ø±=our_price,
                Ø§Ù„Ù…Ø§Ø±ÙƒØ©=brand, Ø§Ù„Ø­Ø¬Ù…=sz_str, Ø§Ù„Ù†ÙˆØ¹=ptype, Ø§Ù„Ø¬Ù†Ø³=gender,
                Ù…Ù†ØªØ¬_Ø§Ù„Ù…Ù†Ø§ÙØ³=best["name"], Ù…Ø¹Ø±Ù_Ø§Ù„Ù…Ù†Ø§ÙØ³=best.get("product_id",""),
                Ø³Ø¹Ø±_Ø§Ù„Ù…Ù†Ø§ÙØ³=cp, Ø§Ù„ÙØ±Ù‚=diff, Ù†Ø³Ø¨Ø©_Ø§Ù„ØªØ·Ø§Ø¨Ù‚=score, Ø«Ù‚Ø©_AI=ai_lbl,
                Ø§Ù„Ù‚Ø±Ø§Ø±=dec, Ø§Ù„Ø®Ø·ÙˆØ±Ø©=risk, Ø§Ù„Ù…Ù†Ø§ÙØ³=best.get("competitor",""),
                Ø¹Ø¯Ø¯_Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†=len({c.get("competitor","") for c in ac}),
                Ø¬Ù…ÙŠØ¹_Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†=ac, Ù…ØµØ¯Ø±_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©=src or "fuzzy",
                ØªØ§Ø±ÙŠØ®_Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©=datetime.now().strftime("%Y-%m-%d"))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ â€” v21 Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def run_full_analysis(our_df, comp_dfs, progress_callback=None, use_ai=True):
    """
    1. Ø¨Ù†Ø§Ø¡ CompIndex Ù„ÙƒÙ„ Ù…Ù†Ø§ÙØ³ (ØªØ·Ø¨ÙŠØ¹ Ù…Ø³Ø¨Ù‚)
    2. Ù„ÙƒÙ„ Ù…Ù†ØªØ¬Ù†Ø§ â†’ search vectorized
    3. scoreâ‰¥97 â†’ ØªÙ„Ù‚Ø§Ø¦ÙŠ | 62-96 â†’ AI batch | <62 â†’ Ù…ÙÙ‚ÙˆØ¯
    """
    results = []
    our_col       = _fcol(our_df, ["Ø§Ù„Ù…Ù†ØªØ¬","Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬","Product","Name","name"])
    our_price_col = _fcol(our_df, ["Ø§Ù„Ø³Ø¹Ø±","Ø³Ø¹Ø±","Price","price","PRICE"])
    our_id_col    = _fcol(our_df, ["ID","id","Ù…Ø¹Ø±Ù","Ø±Ù‚Ù… Ø§Ù„Ù…Ù†ØªØ¬","SKU","sku","Ø§Ù„ÙƒÙˆØ¯"])

    # â”€â”€ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© â”€â”€
    indices = {}
    for cname, cdf in comp_dfs.items():
        ccol = _fcol(cdf, ["Ø§Ù„Ù…Ù†ØªØ¬","Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬","Product","Name","name"])
        icol = _fcol(cdf, ["ID","id","Ù…Ø¹Ø±Ù","Ø±Ù‚Ù… Ø§Ù„Ù…Ù†ØªØ¬","SKU","sku","Ø§Ù„ÙƒÙˆØ¯","code"])
        indices[cname] = CompIndex(cdf, ccol, icol, cname)

    total   = len(our_df)
    pending = []
    BATCH   = 12  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù€ batch Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª API

    def _flush():
        if not pending: return
        idxs = _ai_batch(pending)
        for j, it in enumerate(pending):
            ci = idxs[j] if j<len(idxs) else 0
            if ci < 0:
                results.append(_row(it["product"],it["our_price"],it["our_id"],
                                    it["brand"],it["size"],it["ptype"],it["gender"],
                                    None,"ğŸ”µ Ù…ÙÙ‚ÙˆØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³","gemini_no_match"))
            else:
                best = it["candidates"][ci]
                results.append(_row(it["product"],it["our_price"],it["our_id"],
                                    it["brand"],it["size"],it["ptype"],it["gender"],
                                    best,src="gemini",all_cands=it["all_cands"]))
        pending.clear()

    for i, (_, row) in enumerate(our_df.iterrows()):
        product = str(row.get(our_col,"")).strip()
        if not product or is_sample(product):
            if progress_callback: progress_callback((i+1)/total)
            continue

        our_price = 0.0
        if our_price_col:
            try: our_price = float(str(row[our_price_col]).replace(",",""))
            except: pass

        our_id  = _pid(row, our_id_col)
        brand   = extract_brand(product)
        size    = extract_size(product)
        ptype   = extract_type(product)
        gender  = extract_gender(product)
        our_n   = normalize(product)

        # â”€â”€ Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù…Ù† ÙƒÙ„ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ â”€â”€
        all_cands = []
        for idx_obj in indices.values():
            all_cands.extend(idx_obj.search(our_n, brand, size, ptype, gender, top_n=5))

        if not all_cands:
            results.append(_row(product,our_price,our_id,brand,size,ptype,gender,
                                None,"ğŸ”µ Ù…ÙÙ‚ÙˆØ¯ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù†Ø§ÙØ³"))
            if progress_callback: progress_callback((i+1)/total)
            continue

        all_cands.sort(key=lambda x: x["score"], reverse=True)
        top5  = all_cands[:5]
        best0 = top5[0]

        if best0["score"] >= 97 or not use_ai:
            # ÙˆØ§Ø¶Ø­ ØªÙ…Ø§Ù…Ø§Ù‹ â†’ Ù„Ø§ Ø­Ø§Ø¬Ø© AI
            results.append(_row(product,our_price,our_id,brand,size,ptype,gender,
                                best0,src="auto",all_cands=all_cands))
        else:
            # ØºØ§Ù…Ø¶ â†’ AI batch
            pending.append(dict(product=product,our_price=our_price,our_id=our_id,
                                brand=brand,size=size,ptype=ptype,gender=gender,
                                candidates=top5,all_cands=all_cands,
                                our=product,price=our_price))
            if len(pending) >= BATCH: _flush()

        if progress_callback: progress_callback((i+1)/total)

    _flush()
    return pd.DataFrame(results)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def find_missing_products(our_df, comp_dfs):
    our_col  = _fcol(our_df, ["Ø§Ù„Ù…Ù†ØªØ¬","Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬","Product","Name","name"])
    our_norms = [normalize(str(r.get(our_col,"")))
                 for _,r in our_df.iterrows()
                 if not is_sample(str(r.get(our_col,"")))]

    missing, seen = [], set()
    for cname, cdf in comp_dfs.items():
        ccol = _fcol(cdf, ["Ø§Ù„Ù…Ù†ØªØ¬","Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬","Product","Name","name"])
        icol = _fcol(cdf, ["ID","id","Ù…Ø¹Ø±Ù","Ø±Ù‚Ù… Ø§Ù„Ù…Ù†ØªØ¬","SKU","sku","Ø§Ù„ÙƒÙˆØ¯","code"])
        for _, row in cdf.iterrows():
            cp = str(row.get(ccol,"")).strip()
            if not cp or is_sample(cp): continue
            cn = normalize(cp)
            if not cn or cn in seen: continue
            match = rf_process.extractOne(cn, our_norms, scorer=fuzz.token_sort_ratio, score_cutoff=70)
            if match: continue
            seen.add(cn)
            sz = extract_size(cp)
            missing.append({
                "Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§ÙØ³": cp, "Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ù†Ø§ÙØ³": _pid(row,icol),
                "Ø³Ø¹Ø± Ø§Ù„Ù…Ù†Ø§ÙØ³": _price(row), "Ø§Ù„Ù…Ù†Ø§ÙØ³": cname,
                "Ø§Ù„Ù…Ø§Ø±ÙƒØ©": extract_brand(cp),
                "Ø§Ù„Ø­Ø¬Ù…": f"{int(sz)}ml" if sz else "",
                "Ø§Ù„Ù†ÙˆØ¹": extract_type(cp), "Ø§Ù„Ø¬Ù†Ø³": extract_gender(cp),
                "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø±ØµØ¯": datetime.now().strftime("%Y-%m-%d"),
            })
    return pd.DataFrame(missing) if missing else pd.DataFrame()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ØªØµØ¯ÙŠØ± Excel Ù…Ù„ÙˆÙ‘Ù†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def export_excel(df, sheet_name="Ø§Ù„Ù†ØªØ§Ø¦Ø¬"):
    from openpyxl.styles import PatternFill, Font, Alignment
    from openpyxl.utils import get_column_letter
    output = io.BytesIO()
    edf = df.copy()
    for col in ["Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†","Ø¬Ù…ÙŠØ¹_Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†"]:
        if col in edf.columns: edf = edf.drop(columns=[col])
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        edf.to_excel(writer, sheet_name=sheet_name[:31], index=False)
        ws = writer.sheets[sheet_name[:31]]
        hfill = PatternFill("solid", fgColor="1a1a2e")
        hfont = Font(color="FFFFFF", bold=True, size=10)
        for cell in ws[1]:
            cell.fill=hfill; cell.font=hfont
            cell.alignment=Alignment(horizontal="center")
        COLORS = {"ğŸ”´ Ø³Ø¹Ø± Ø£Ø¹Ù„Ù‰":"FFCCCC","ğŸŸ¢ Ø³Ø¹Ø± Ø£Ù‚Ù„":"CCFFCC",
                  "âœ… Ù…ÙˆØ§ÙÙ‚":"CCFFEE","âš ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø©":"FFF3CC","ğŸ”µ Ù…ÙÙ‚ÙˆØ¯":"CCE5FF"}
        dcol = None
        for i, cell in enumerate(ws[1], 1):
            if cell.value and "Ø§Ù„Ù‚Ø±Ø§Ø±" in str(cell.value): dcol=i; break
        if dcol:
            for ri, row in enumerate(ws.iter_rows(min_row=2), 2):
                val = str(ws.cell(ri,dcol).value or "")
                for k,c in COLORS.items():
                    if k.split()[0] in val:
                        for cell in row: cell.fill=PatternFill("solid",fgColor=c)
                        break
        for ci, col in enumerate(ws.columns, 1):
            w = max(len(str(c.value or "")) for c in col)
            ws.column_dimensions[get_column_letter(ci)].width = min(w+4, 55)
    return output.getvalue()

def export_section_excel(df, sname):
    return export_excel(df, sheet_name=sname[:31])
